# Hackathon Rules, Problem Statements & Judging

## Hackathon Rules

To keep things fair and aligned with our goals, all teams must follow these rules:

- **Open Source:** Everything shown in the demo must be fully open source. This includes every component - backend, frontend, models, and any other parts of the project - published under an approved open source license.

- **New Work Only:** All projects must be started from scratch during the hackathon with no previous work.

- **Team Size:** Teams may have up to **2** members.

- **Banned Projects:** Projects will be disqualified if they: violate legal, ethical, or platform policies, use code, data, or assets you do not have the rights to.

---

## Problem Statements & Example Projects

### Problem Statement One: Build a Tool That Should Exist
Create the AI-native app or workflow you wish someone had already built. Eliminate busywork. Make hard things effortless.

**Example Projects:**
- Contract Lifecycle Autopilot - Extracts all obligations from contracts and tracks deadlines with automatic reminders
- Product Changelog Publisher - Turns release notes into customer-facing announcements across multiple channels
- Bug Report Enricher - Automatically adds system logs, user history, and reproduction steps to support tickets

### Problem Statement Two: Break the Barriers
Expert knowledge, essential tools, AI's benefits — take something powerful that's locked behind expertise, cost, language, or infrastructure and put it in everyone's hands.

**Example Projects:**
- Crop Doctor — Combines image analysis, weather data, and soil reports to diagnose plant diseases and recommend organic treatment protocols.
- Accessibility Auditor — Evaluates websites, documents, and physical spaces against accessibility standards and generates remediation plans.
- Open Source Hardware Guide — Helps makers navigate component selection, PCB design, and manufacturing for hardware projects.

### Problem Statement Three: Amplify Human Judgment ← OUR TARGET
Build AI that makes researchers, professionals, and decision-makers dramatically more capable — without taking them out of the loop. The best AI doesn't replace human expertise. It sharpens it.

**Example Projects:**
- Brand Safety Monitor - Reviews ad placements and content adjacencies, flagging reputation risks for marketing teams
- Discovery Anomaly Detector - Flags documents in discovery that should exist but are missing based on references in other documents
- Grading Calibration Partner — Highlights scoring inconsistencies in instructor assessments, supporting but not overriding professional judgment on standards.

---

## Judging Criteria

| Criterion | Weight | Our Angle |
|---|---|---|
| **Demo** | 30% | Live call → structured report → 4-week trend → escalation. Judges think about their own parents. Emotional + technical. |
| **Opus 4.6 Use** | 25% | Deep: runs structured clinical protocol conversationally, maintains personality + memory across calls, scores validated instruments, detects longitudinal cognitive trends, writes nuanced caregiver summaries, handles branching logic (PHQ-2 → C-SSRS). Not just "API call." |
| **Impact** | 25% | 14M isolated seniors. 60% of dementia undiagnosed. Existing workforce can't scale. CLOVA CareCall proves model works but doesn't exist in US. |
| **Depth & Execution** | 20% | Evidence-based protocol from published research. Free instruments only. Three-tier escalation. Longitudinal tracking. Clear product architecture. Shows real iteration and craft. |

---

## Prizes

- **1st Place:** $50,000 in Claude API Credits
- **2nd Place:** $30,000 in Claude API Credits
- **3rd Place:** $10,000 in Claude API Credits
- **Most Creative Opus 4.6 Exploration:** $5,000 in Claude API Credits
  - For the team that found the most interesting edge of this new model — the unexpected capability or the use case nobody thought to try. We want the project that taught us something new about what Opus 4.6 can do.
- **The "Keep Thinking" Prize:** $5,000 in Claude API Credits
  - For the project that didn't stop at the first idea. We're looking for the team that pushed past the obvious, iterated relentlessly, and showed the kind of depth that turns a good hack into something genuinely surprising.

---

## Strategic Notes

**Primary fit: Problem Statement 3 (Amplify Human Judgment)**
- AI screens at scale, humans make all decisions
- Caregivers, PCPs, and AAAs are the decision-makers — we make them dramatically more capable by feeding them signal they'd never otherwise get
- The AI never diagnoses, never treats, never decides — it surfaces structured findings so professionals can act

**Secondary fit: Problem Statement 2 (Break the Barriers)**
- Clinical screening is locked behind workforce scarcity and cost
- We put validated instruments into a phone call that costs $0.68/person/month

**Special prize angles:**
- "Most Creative Opus 4.6 Exploration" — using Opus 4.6 as a conversational clinical protocol engine with longitudinal memory, not just a chatbot
- "Keep Thinking" — deep research iteration, honest evidence critique (Kahlon takedown → HEAL-HOA pivot → free-instrument-only protocol)
